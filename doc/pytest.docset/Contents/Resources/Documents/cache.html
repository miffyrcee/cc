


<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Cache: working with cross-testrun state &#8212; pytest documentation</title>
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/basic.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/pygments_pytest.css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <link rel="shortcut icon" href="_static/favicon.png"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="unittest.TestCase Support" href="unittest.html" />
    <link rel="prev" title="Parametrizing fixtures and test functions" href="parametrize.html" />
  <script>DOCUMENTATION_OPTIONS.URL_ROOT = './';</script>
   
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="unittest.html" title="unittest.TestCase Support"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="parametrize.html" title="Parametrizing fixtures and test functions"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="contents.html">pytest-6.2</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Cache: working with cross-testrun state</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
          <div class="body" role="main">
            
  <div class="section" id="cache-working-with-cross-testrun-state">
<span id="cache"></span><span id="cache-provider"></span><h1>Cache: working with cross-testrun state<a class="headerlink" href="#cache-working-with-cross-testrun-state" title="Permalink to this headline">¶</a></h1>
<div class="section" id="usage">
<h2>Usage<a class="headerlink" href="#usage" title="Permalink to this headline">¶</a></h2>
<p>The plugin provides two command line options to rerun failures from the
last <code class="docutils literal notranslate"><span class="pre">pytest</span></code> invocation:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--lf</span></code>, <code class="docutils literal notranslate"><span class="pre">--last-failed</span></code> - to only re-run the failures.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--ff</span></code>, <code class="docutils literal notranslate"><span class="pre">--failed-first</span></code> - to run the failures first and then the rest of
the tests.</p></li>
</ul>
<p>For cleanup (usually not needed), a <code class="docutils literal notranslate"><span class="pre">--cache-clear</span></code> option allows to remove
all cross-session cache contents ahead of a test run.</p>
<p>Other plugins may access the <a class="reference internal" href="#config-cache">config.cache</a> object to set/get
<strong>json encodable</strong> values between <code class="docutils literal notranslate"><span class="pre">pytest</span></code> invocations.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This plugin is enabled by default, but can be disabled if needed: see
<a class="reference internal" href="plugins.html#cmdunregister"><span class="std std-ref">Deactivating / unregistering a plugin by name</span></a> (the internal name for this plugin is
<code class="docutils literal notranslate"><span class="pre">cacheprovider</span></code>).</p>
</div>
</div>
<div class="section" id="rerunning-only-failures-or-failures-first">
<h2>Rerunning only failures or failures first<a class="headerlink" href="#rerunning-only-failures-or-failures-first" title="Permalink to this headline">¶</a></h2>
<p>First, let’s create 50 test invocation of which only 2 fail:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># content of test_50.py</span>
<span class="kn">import</span> <span class="nn">pytest</span>


<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">parametrize</span><span class="p">(</span><span class="s2">&quot;i&quot;</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">test_num</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">17</span><span class="p">,</span> <span class="mi">25</span><span class="p">):</span>
        <span class="n">pytest</span><span class="o">.</span><span class="n">fail</span><span class="p">(</span><span class="s2">&quot;bad luck&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>If you run this for the first time you will see two failures:</p>
<div class="highlight-pytest notranslate"><div class="highlight"><pre><span></span>$ pytest -q
<span class=" -Color -Color-Green">.................</span><span class=" -Color -Color-Red">F</span><span class=" -Color -Color-Green">.......</span><span class=" -Color -Color-Red">F</span><span class=" -Color -Color-Green">........................</span>                   <span class=" -Color -Color-Red">[100%]</span>
================================= FAILURES =================================
<span class=" -Color -Color-Bold -Color-Bold-Red">_______________________________ test_num[17] _______________________________</span>

i = 17

    @pytest.mark.parametrize(&quot;i&quot;, range(50))
    def test_num(i):
        if i in (17, 25):
&gt;           pytest.fail(&quot;bad luck&quot;)
<span class=" -Color -Color-Bold -Color-Bold-Red">E           Failed: bad luck</span>

<span class=" -Color -Color-Bold -Color-Bold-Red">test_50.py</span>:7: Failed
<span class=" -Color -Color-Bold -Color-Bold-Red">_______________________________ test_num[25] _______________________________</span>

i = 25

    @pytest.mark.parametrize(&quot;i&quot;, range(50))
    def test_num(i):
        if i in (17, 25):
&gt;           pytest.fail(&quot;bad luck&quot;)
<span class=" -Color -Color-Bold -Color-Bold-Red">E           Failed: bad luck</span>

<span class=" -Color -Color-Bold -Color-Bold-Red">test_50.py</span>:7: Failed
========================= short test summary info ==========================
FAILED test_50.py::test_num[17] - Failed: bad luck
FAILED test_50.py::test_num[25] - Failed: bad luck
<span class=" -Color -Color-Bold -Color-Bold-Red">2 failed</span>, <span class=" -Color -Color-Green">48 passed</span><span class=" -Color -Color-Red"> in 0.12s</span>
</pre></div>
</div>
<p>If you then run it with <code class="docutils literal notranslate"><span class="pre">--lf</span></code>:</p>
<div class="highlight-pytest notranslate"><div class="highlight"><pre><span></span>$ pytest --lf
<span class=" -Color -Color-Bold">=========================== test session starts ============================</span>
platform linux -- Python 3.x.y, pytest-6.x.y, py-1.x.y, pluggy-1.x.y
cachedir: $PYTHON_PREFIX/.pytest_cache
rootdir: $REGENDOC_TMPDIR
collected 2 items
run-last-failure: rerun previous 2 failures

test_50.py <span class=" -Color -Color-Red">FF</span>                                                        <span class=" -Color -Color-Red">[100%]</span>

================================= FAILURES =================================
<span class=" -Color -Color-Bold -Color-Bold-Red">_______________________________ test_num[17] _______________________________</span>

i = 17

    @pytest.mark.parametrize(&quot;i&quot;, range(50))
    def test_num(i):
        if i in (17, 25):
&gt;           pytest.fail(&quot;bad luck&quot;)
<span class=" -Color -Color-Bold -Color-Bold-Red">E           Failed: bad luck</span>

<span class=" -Color -Color-Bold -Color-Bold-Red">test_50.py</span>:7: Failed
<span class=" -Color -Color-Bold -Color-Bold-Red">_______________________________ test_num[25] _______________________________</span>

i = 25

    @pytest.mark.parametrize(&quot;i&quot;, range(50))
    def test_num(i):
        if i in (17, 25):
&gt;           pytest.fail(&quot;bad luck&quot;)
<span class=" -Color -Color-Bold -Color-Bold-Red">E           Failed: bad luck</span>

<span class=" -Color -Color-Bold -Color-Bold-Red">test_50.py</span>:7: Failed
========================= short test summary info ==========================
FAILED test_50.py::test_num[17] - Failed: bad luck
FAILED test_50.py::test_num[25] - Failed: bad luck
<span class=" -Color -Color-Red">============================ </span><span class=" -Color -Color-Bold -Color-Bold-Red">2 failed</span><span class=" -Color -Color-Red"> in 0.12s =============================</span>
</pre></div>
</div>
<p>You have run only the two failing tests from the last run, while the 48 passing
tests have not been run (“deselected”).</p>
<p>Now, if you run with the <code class="docutils literal notranslate"><span class="pre">--ff</span></code> option, all tests will be run but the first
previous failures will be executed first (as can be seen from the series
of <code class="docutils literal notranslate"><span class="pre">FF</span></code> and dots):</p>
<div class="highlight-pytest notranslate"><div class="highlight"><pre><span></span>$ pytest --ff
<span class=" -Color -Color-Bold">=========================== test session starts ============================</span>
platform linux -- Python 3.x.y, pytest-6.x.y, py-1.x.y, pluggy-1.x.y
cachedir: $PYTHON_PREFIX/.pytest_cache
rootdir: $REGENDOC_TMPDIR
collected 50 items
run-last-failure: rerun previous 2 failures first

test_50.py <span class=" -Color -Color-Red">FF</span><span class=" -Color -Color-Green">................................................</span>        <span class=" -Color -Color-Red">[100%]</span>

================================= FAILURES =================================
<span class=" -Color -Color-Bold -Color-Bold-Red">_______________________________ test_num[17] _______________________________</span>

i = 17

    @pytest.mark.parametrize(&quot;i&quot;, range(50))
    def test_num(i):
        if i in (17, 25):
&gt;           pytest.fail(&quot;bad luck&quot;)
<span class=" -Color -Color-Bold -Color-Bold-Red">E           Failed: bad luck</span>

<span class=" -Color -Color-Bold -Color-Bold-Red">test_50.py</span>:7: Failed
<span class=" -Color -Color-Bold -Color-Bold-Red">_______________________________ test_num[25] _______________________________</span>

i = 25

    @pytest.mark.parametrize(&quot;i&quot;, range(50))
    def test_num(i):
        if i in (17, 25):
&gt;           pytest.fail(&quot;bad luck&quot;)
<span class=" -Color -Color-Bold -Color-Bold-Red">E           Failed: bad luck</span>

<span class=" -Color -Color-Bold -Color-Bold-Red">test_50.py</span>:7: Failed
========================= short test summary info ==========================
FAILED test_50.py::test_num[17] - Failed: bad luck
FAILED test_50.py::test_num[25] - Failed: bad luck
<span class=" -Color -Color-Red">======================= </span><span class=" -Color -Color-Bold -Color-Bold-Red">2 failed</span>, <span class=" -Color -Color-Green">48 passed</span><span class=" -Color -Color-Red"> in 0.12s =======================</span>
</pre></div>
</div>
<p id="config-cache">New <code class="docutils literal notranslate"><span class="pre">--nf</span></code>, <code class="docutils literal notranslate"><span class="pre">--new-first</span></code> options: run new tests first followed by the rest
of the tests, in both cases tests are also sorted by the file modified time,
with more recent files coming first.</p>
</div>
<div class="section" id="behavior-when-no-tests-failed-in-the-last-run">
<h2>Behavior when no tests failed in the last run<a class="headerlink" href="#behavior-when-no-tests-failed-in-the-last-run" title="Permalink to this headline">¶</a></h2>
<p>When no tests failed in the last run, or when no cached <code class="docutils literal notranslate"><span class="pre">lastfailed</span></code> data was
found, <code class="docutils literal notranslate"><span class="pre">pytest</span></code> can be configured either to run all of the tests or no tests,
using the <code class="docutils literal notranslate"><span class="pre">--last-failed-no-failures</span></code> option, which takes one of the following values:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest --last-failed --last-failed-no-failures all    <span class="c1"># run all tests (default behavior)</span>
pytest --last-failed --last-failed-no-failures none   <span class="c1"># run no tests and exit</span>
</pre></div>
</div>
</div>
<div class="section" id="the-new-config-cache-object">
<h2>The new config.cache object<a class="headerlink" href="#the-new-config-cache-object" title="Permalink to this headline">¶</a></h2>
<p>Plugins or conftest.py support code can get a cached value using the
pytest <code class="docutils literal notranslate"><span class="pre">config</span></code> object.  Here is a basic example plugin which
implements a <a class="reference internal" href="fixture.html#fixture"><span class="std std-ref">fixture</span></a> which re-uses previously created state
across pytest invocations:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># content of test_caching.py</span>
<span class="kn">import</span> <span class="nn">pytest</span>
<span class="kn">import</span> <span class="nn">time</span>


<span class="k">def</span> <span class="nf">expensive_computation</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;running expensive computation...&quot;</span><span class="p">)</span>


<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span> <span class="nf">mydata</span><span class="p">(</span><span class="n">request</span><span class="p">):</span>
    <span class="n">val</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">cache</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;example/value&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">val</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">expensive_computation</span><span class="p">()</span>
        <span class="n">val</span> <span class="o">=</span> <span class="mi">42</span>
        <span class="n">request</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">cache</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;example/value&quot;</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">val</span>


<span class="k">def</span> <span class="nf">test_function</span><span class="p">(</span><span class="n">mydata</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">mydata</span> <span class="o">==</span> <span class="mi">23</span>
</pre></div>
</div>
<p>If you run this command for the first time, you can see the print statement:</p>
<div class="highlight-pytest notranslate"><div class="highlight"><pre><span></span>$ pytest -q
<span class=" -Color -Color-Red">F</span>                                                                    <span class=" -Color -Color-Red">[100%]</span>
================================= FAILURES =================================
<span class=" -Color -Color-Bold -Color-Bold-Red">______________________________ test_function _______________________________</span>

mydata = 42

    def test_function(mydata):
&gt;       assert mydata == 23
<span class=" -Color -Color-Bold -Color-Bold-Red">E       assert 42 == 23</span>

<span class=" -Color -Color-Bold -Color-Bold-Red">test_caching.py</span>:20: AssertionError
-------------------------- Captured stdout setup ---------------------------
running expensive computation...
========================= short test summary info ==========================
FAILED test_caching.py::test_function - assert 42 == 23
<span class=" -Color -Color-Bold -Color-Bold-Red">1 failed</span><span class=" -Color -Color-Red"> in 0.12s</span>
</pre></div>
</div>
<p>If you run it a second time, the value will be retrieved from
the cache and nothing will be printed:</p>
<div class="highlight-pytest notranslate"><div class="highlight"><pre><span></span>$ pytest -q
<span class=" -Color -Color-Red">F</span>                                                                    <span class=" -Color -Color-Red">[100%]</span>
================================= FAILURES =================================
<span class=" -Color -Color-Bold -Color-Bold-Red">______________________________ test_function _______________________________</span>

mydata = 42

    def test_function(mydata):
&gt;       assert mydata == 23
<span class=" -Color -Color-Bold -Color-Bold-Red">E       assert 42 == 23</span>

<span class=" -Color -Color-Bold -Color-Bold-Red">test_caching.py</span>:20: AssertionError
========================= short test summary info ==========================
FAILED test_caching.py::test_function - assert 42 == 23
<span class=" -Color -Color-Bold -Color-Bold-Red">1 failed</span><span class=" -Color -Color-Red"> in 0.12s</span>
</pre></div>
</div>
<p>See the <a class="reference internal" href="reference.html#std-fixture-cache"><code class="xref std std-fixture docutils literal notranslate"><span class="pre">config.cache</span> <span class="pre">fixture</span></code></a> for more details.</p>
</div>
<div class="section" id="inspecting-cache-content">
<h2>Inspecting Cache content<a class="headerlink" href="#inspecting-cache-content" title="Permalink to this headline">¶</a></h2>
<p>You can always peek at the content of the cache using the
<code class="docutils literal notranslate"><span class="pre">--cache-show</span></code> command line option:</p>
<div class="highlight-pytest notranslate"><div class="highlight"><pre><span></span>$ pytest --cache-show
<span class=" -Color -Color-Bold">=========================== test session starts ============================</span>
platform linux -- Python 3.x.y, pytest-6.x.y, py-1.x.y, pluggy-1.x.y
cachedir: $PYTHON_PREFIX/.pytest_cache
rootdir: $REGENDOC_TMPDIR
cachedir: $PYTHON_PREFIX/.pytest_cache
--------------------------- cache values for &#39;*&#39; ---------------------------
cache/lastfailed contains:
  {&#39;test_50.py::test_num[17]&#39;: True,
   &#39;test_50.py::test_num[25]&#39;: True,
   &#39;test_assert1.py::test_function&#39;: True,
   &#39;test_assert2.py::test_set_comparison&#39;: True,
   &#39;test_caching.py::test_function&#39;: True,
   &#39;test_foocompare.py::test_compare&#39;: True}
cache/nodeids contains:
  [&#39;test_50.py::test_num[0]&#39;,
   &#39;test_50.py::test_num[10]&#39;,
   &#39;test_50.py::test_num[11]&#39;,
   &#39;test_50.py::test_num[12]&#39;,
   &#39;test_50.py::test_num[13]&#39;,
   &#39;test_50.py::test_num[14]&#39;,
   &#39;test_50.py::test_num[15]&#39;,
   &#39;test_50.py::test_num[16]&#39;,
   &#39;test_50.py::test_num[17]&#39;,
   &#39;test_50.py::test_num[18]&#39;,
   &#39;test_50.py::test_num[19]&#39;,
   &#39;test_50.py::test_num[1]&#39;,
   &#39;test_50.py::test_num[20]&#39;,
   &#39;test_50.py::test_num[21]&#39;,
   &#39;test_50.py::test_num[22]&#39;,
   &#39;test_50.py::test_num[23]&#39;,
   &#39;test_50.py::test_num[24]&#39;,
   &#39;test_50.py::test_num[25]&#39;,
   &#39;test_50.py::test_num[26]&#39;,
   &#39;test_50.py::test_num[27]&#39;,
   &#39;test_50.py::test_num[28]&#39;,
   &#39;test_50.py::test_num[29]&#39;,
   &#39;test_50.py::test_num[2]&#39;,
   &#39;test_50.py::test_num[30]&#39;,
   &#39;test_50.py::test_num[31]&#39;,
   &#39;test_50.py::test_num[32]&#39;,
   &#39;test_50.py::test_num[33]&#39;,
   &#39;test_50.py::test_num[34]&#39;,
   &#39;test_50.py::test_num[35]&#39;,
   &#39;test_50.py::test_num[36]&#39;,
   &#39;test_50.py::test_num[37]&#39;,
   &#39;test_50.py::test_num[38]&#39;,
   &#39;test_50.py::test_num[39]&#39;,
   &#39;test_50.py::test_num[3]&#39;,
   &#39;test_50.py::test_num[40]&#39;,
   &#39;test_50.py::test_num[41]&#39;,
   &#39;test_50.py::test_num[42]&#39;,
   &#39;test_50.py::test_num[43]&#39;,
   &#39;test_50.py::test_num[44]&#39;,
   &#39;test_50.py::test_num[45]&#39;,
   &#39;test_50.py::test_num[46]&#39;,
   &#39;test_50.py::test_num[47]&#39;,
   &#39;test_50.py::test_num[48]&#39;,
   &#39;test_50.py::test_num[49]&#39;,
   &#39;test_50.py::test_num[4]&#39;,
   &#39;test_50.py::test_num[5]&#39;,
   &#39;test_50.py::test_num[6]&#39;,
   &#39;test_50.py::test_num[7]&#39;,
   &#39;test_50.py::test_num[8]&#39;,
   &#39;test_50.py::test_num[9]&#39;,
   &#39;test_assert1.py::test_function&#39;,
   &#39;test_assert2.py::test_set_comparison&#39;,
   &#39;test_caching.py::test_function&#39;,
   &#39;test_foocompare.py::test_compare&#39;]
cache/stepwise contains:
  []
example/value contains:
  42

<span class=" -Color -Color-Yellow">========================== no tests ran in 0.12s ===========================</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">--cache-show</span></code> takes an optional argument to specify a glob pattern for
filtering:</p>
<div class="highlight-pytest notranslate"><div class="highlight"><pre><span></span>$ pytest --cache-show example/*
<span class=" -Color -Color-Bold">=========================== test session starts ============================</span>
platform linux -- Python 3.x.y, pytest-6.x.y, py-1.x.y, pluggy-1.x.y
cachedir: $PYTHON_PREFIX/.pytest_cache
rootdir: $REGENDOC_TMPDIR
cachedir: $PYTHON_PREFIX/.pytest_cache
----------------------- cache values for &#39;example/*&#39; -----------------------
example/value contains:
  42

<span class=" -Color -Color-Yellow">========================== no tests ran in 0.12s ===========================</span>
</pre></div>
</div>
</div>
<div class="section" id="clearing-cache-content">
<h2>Clearing Cache content<a class="headerlink" href="#clearing-cache-content" title="Permalink to this headline">¶</a></h2>
<p>You can instruct pytest to clear all cache files and values
by adding the <code class="docutils literal notranslate"><span class="pre">--cache-clear</span></code> option like this:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest --cache-clear
</pre></div>
</div>
<p>This is recommended for invocations from Continuous Integration
servers where isolation and correctness is more important
than speed.</p>
</div>
<div class="section" id="stepwise">
<h2>Stepwise<a class="headerlink" href="#stepwise" title="Permalink to this headline">¶</a></h2>
<p>As an alternative to <code class="docutils literal notranslate"><span class="pre">--lf</span> <span class="pre">-x</span></code>, especially for cases where you expect a large part of the test suite will fail, <code class="docutils literal notranslate"><span class="pre">--sw</span></code>, <code class="docutils literal notranslate"><span class="pre">--stepwise</span></code> allows you to fix them one at a time. The test suite will run until the first failure and then stop. At the next invocation, tests will continue from the last failing test and then run until the next failing test. You may use the <code class="docutils literal notranslate"><span class="pre">--stepwise-skip</span></code> option to ignore one failing test and stop the test execution on the second failing test instead. This is useful if you get stuck on a failing test and just want to ignore it until later.</p>
</div>
</div>


            <div class="clearer"></div>
          </div>
      </div>
  <span id="sidebar-top"></span>
      <div class="clearer"></div>
    </div>
  
    <div class="footer" role="contentinfo">
        &#169; Copyright 2015–2020, holger krekel and pytest-dev team.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 3.5.4.
    </div>
  <script src="_static/version_warning_offset.js"></script>

  </body>
</html>